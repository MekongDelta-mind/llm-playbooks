{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetuning a Gemma 2B chat model for Q & A\n",
    "\n",
    "https://medium.com/the-ai-forum/instruction-fine-tuning-gemma-2b-on-medical-reasoning-and-convert-the-finetuned-model-into-gguf-844191f8d329"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_LIMIT = 1024\n",
    "TRAIN_SIZE = 1000\n",
    "TEST_SIZE = 100\n",
    "EVAL_SIZE = 50\n",
    "SEED = 123\n",
    "\n",
    "BASE_MODEL_ID = \"google/gemma-2-9b-it\"\n",
    "USE_4BIT = True\n",
    "USE_8BIT = False\n",
    "GENERATE_KWARGS = dict(\n",
    "    do_sample=True,\n",
    "    max_new_tokens=512,\n",
    "    temperature=1e-3,\n",
    ")\n",
    "EVAL_BATCH_SIZE = 8\n",
    "\n",
    "TRAIN_MAX_LENGTH = 512\n",
    "TRAIN_NUM_EPOCHS = 1\n",
    "TRAIN_BATCH_SIZE = 4\n",
    "TRAIN_GRADIENT_ACCUMULATION_STEPS = 1\n",
    "TRAIN_LOGGING_STEPS = 10\n",
    "EVAL_ACCUMULATION_STEPS = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Gemma 2B instruct model\n",
    "\n",
    "https://huggingface.co/google/gemma-2b-it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d960075556e4196a13e6a18ddf2ebcf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_ID)\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=USE_4BIT,\n",
    "    load_in_8bit=USE_8BIT,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL_ID,\n",
    "    attn_implementation='eager',\n",
    "    quantization_config=quantization_config\n",
    ")\n",
    "print(model.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Medical Q & A\n",
    "\n",
    "https://huggingface.co/datasets/medalpaca/medical_meadow_medqa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b309c2ee95324ff3954dc77ada100d42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/10178 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input', 'output'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input', 'output'],\n",
       "        num_rows: 100\n",
       "    })\n",
       "})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# input\n",
      "Q:A 45-year-old man comes to the physician because of worsening shortness of breath and dry cough for 6 months. The patient's symptoms get worse when he walks more than about 150 yards. He also reports fatigue and difficulty swallowing solid foods. In cold weather, his fingers occasionally turn blue and become painful. He occasionally smokes cigarettes on weekends. His temperature is 37째C (98.6째F), and respirations are 22/min, pulse is 87/min, and blood pressure is 126/85 mm Hg. The skin over his trunk and arms is thickened and tightened. Fine inspiratory crackles are heard over bilateral lower lung fields on auscultation. Which of the following additional findings is most likely in this patient?? \n",
      "{'A': 'Decreased right atrial pressure', 'B': 'Increased lung compliance', 'C': 'Decreased diffusing capacity', 'D': 'Increased airway resistance', 'E': 'Decreased A-a gradient'},\n",
      "\n",
      "# output\n",
      "C: Decreased diffusing capacity\n"
     ]
    }
   ],
   "source": [
    "from medqa_data import load_train_test_data\n",
    "\n",
    "dataset = load_train_test_data(\n",
    "    train_size=TRAIN_SIZE,\n",
    "    test_size=TEST_SIZE,\n",
    "    seed=SEED,\n",
    "    input_limit=INPUT_LIMIT,\n",
    ")\n",
    "display(dataset)\n",
    "\n",
    "def print_sample(sample: dict[str, str]):\n",
    "    print(\"\\n\".join(f\"\\n# {k}\\n{v}\" for k, v in sample.items())[1:])\n",
    "\n",
    "print_sample(dataset[\"test\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos><start_of_turn>user\n",
      "Q:A child is in the nursery one day after birth. A nurse notices a urine-like discharge being expressed through the umbilical stump. What two structures in the embryo are connected by the structure that failed to obliterate during the embryologic development of this child??\n",
      "{'A': 'Pulmonary artery - aorta', 'B': 'Bladder - yolk sac', 'C': 'Bladder - small bowel', 'D': 'Liver - umbilical vein', 'E': 'Kidney - large bowel'},\n",
      "Give your answer as a JSON dictionary with the \"option\" (a letter from A-E) and the  corresponding\"option_text\". No yapping.<end_of_turn>\n",
      "<start_of_turn>model\n",
      "```json\n",
      "{\n",
      "\"option\": \"B\",\n",
      "\"option_text\": \"Bladder - yolk sac\"\n",
      "}\n",
      "```<end_of_turn><eos>\n"
     ]
    }
   ],
   "source": [
    "question = \"\"\"\n",
    "Q:A child is in the nursery one day after birth. A nurse notices a urine-like discharge being expressed through the umbilical stump. What two structures in the embryo are connected by the structure that failed to obliterate during the embryologic development of this child??\n",
    "{'A': 'Pulmonary artery - aorta', 'B': 'Bladder - yolk sac', 'C': 'Bladder - small bowel', 'D': 'Liver - umbilical vein', 'E': 'Kidney - large bowel'},\n",
    "Give your answer as a JSON dictionary with the \"option\" (a letter from A-E) and the  corresponding\"option_text\". No yapping.\n",
    "\"\"\".strip()\n",
    "\n",
    "chat = [{\"role\": \"user\", \"content\": question}]\n",
    "input_ids = tokenizer.apply_chat_template(chat, add_generation_prompt=True, return_tensors=\"pt\").to(model.device)\n",
    "output_ids = model.generate(input_ids, **GENERATE_KWARGS)\n",
    "input_ids = input_ids.to(\"cpu\")\n",
    "output_ids = output_ids.to(\"cpu\")\n",
    "print(tokenizer.decode(output_ids[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd6de22b1d1741c7af53c203267cdb49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12bc6f2f6cef48e48e2503237fcd6f59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# input\n",
      "Q: A 45-year-old man comes to the physician because of worsening shortness of breath and dry cough for 6 months. The patient's symptoms get worse when he walks more than about 150 yards. He also reports fatigue and difficulty swallowing solid foods. In cold weather, his fingers occasionally turn blue and become painful. He occasionally smokes cigarettes on weekends. His temperature is 37째C (98.6째F), and respirations are 22/min, pulse is 87/min, and blood pressure is 126/85 mm Hg. The skin over his trunk and arms is thickened and tightened. Fine inspiratory crackles are heard over bilateral lower lung fields on auscultation. Which of the following additional findings is most likely in this patient?? \n",
      "{'A': 'Decreased right atrial pressure', 'B': 'Increased lung compliance', 'C': 'Decreased diffusing capacity', 'D': 'Increased airway resistance', 'E': 'Decreased A-a gradient'}\n",
      "Give your answer as a JSON dictionary with the \"option\" (a letter from A-E) and the  corresponding\"option_text\". No yapping.\n",
      "\n",
      "# output\n",
      "{\"option\": \"C\", \"text\": \"Decreased diffusing capacity\"}\n",
      "\n",
      "# true_label\n",
      "C\n"
     ]
    }
   ],
   "source": [
    "from medqa_data import reformat_sample\n",
    "\n",
    "dataset = dataset.map(reformat_sample)\n",
    "train_data, test_data = dataset[\"train\"], dataset[\"test\"]\n",
    "\n",
    "print_sample(test_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "badcb038499d4ea1b5e9c8e61348b130",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gemma accuracy: 45.0%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from medqa_data import create_predict\n",
    "\n",
    "batch_predict = create_predict(tokenizer, model, \"gemma\", batch=True, generate_kwargs=GENERATE_KWARGS)\n",
    "test_data = test_data.map(batch_predict, batched=True, batch_size=EVAL_BATCH_SIZE)\n",
    "\n",
    "gemma_accuracy = (np.asarray(test_data[\"gemma_label\"]) == np.asarray(test_data[\"true_label\"])).mean()\n",
    "print(f\"Gemma accuracy: {round(gemma_accuracy * 100, 1)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised finetuning (SFT)\n",
    "\n",
    "https://huggingface.co/docs/trl/en/sft_trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<start_of_turn>user\\nQ: A 25-year-old sexually active male presents to an internal medicine physician for a routine health check up after having several unprotected sexual encounters. After appropriate testing the physician discusses with the patient that he is HIV+ and must be started on anti-retroviral treatment. Which of the following medications prescribed acts on the gp41 subunit of the HIV envelope glycoprotein?? \\n{\\'A\\': \\'Amantadine\\', \\'B\\': \\'Rimantadine\\', \\'C\\': \\'Zidovudine\\', \\'D\\': \\'Saquinavir\\', \\'E\\': \\'Enfuvirtide\\'}\\nGive your answer as a JSON dictionary with the \"option\" (a letter from A-E) and the  corresponding\"option_text\". No yapping.<end_of_turn>\\n<start_of_turn>model\\n{\"option\": \"E\", \"text\": \"Enfuvirtide\"}<end_of_turn>\\n',\n",
       " '<start_of_turn>user\\nQ: A 53-year-old woman presents with a severe headache, nausea, and vomiting for the past 48 hours. Vitals show a blood pressure of 220/134 mm Hg and a pulse of 88/min. Urinalysis shows a 2+ proteinuria and RBC casts. Which of the following renal lesions is most likely to be seen in this patient?? \\n{\\'A\\': \\'Fibrinoid necrosis\\', \\'B\\': \\'Acute pyelonephritis\\', \\'C\\': \\'Acute tubular necrosis (ATN)\\', \\'D\\': \\'Acute interstitial nephritis (AIN)\\', \\'E\\': \\'Papillary necrosis\\'}\\nGive your answer as a JSON dictionary with the \"option\" (a letter from A-E) and the  corresponding\"option_text\". No yapping.<end_of_turn>\\n<start_of_turn>model\\n{\"option\": \"A\", \"text\": \"Fibrinoid necrosis\"}<end_of_turn>\\n',\n",
       " '<start_of_turn>user\\nQ: Researchers are investigating oncogenes, specifically the KRAS gene that is associated with colon, lung, and pancreatic cancer. They have established that the gain-of-function mutation in this gene increases the chance of cancer development. They are also working to advance the research further to study tumor suppressor genes. Which of the genes below is considered a tumor suppressor gene?? \\n{\\'A\\': \\'JAK2\\', \\'B\\': \\'Her2/neu\\', \\'C\\': \\'Rb\\', \\'D\\': \\'BRAF\\', \\'E\\': \\'BCL-2\\'}\\nGive your answer as a JSON dictionary with the \"option\" (a letter from A-E) and the  corresponding\"option_text\". No yapping.<end_of_turn>\\n<start_of_turn>model\\n{\"option\": \"C\", \"text\": \"Rb\"}<end_of_turn>\\n']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def create_chat_for_finetuning(samples: dict) -> list[str]:\n",
    "    chat_texts = tokenizer.apply_chat_template(\n",
    "        [\n",
    "            [\n",
    "                {\"role\": \"user\", \"content\": input},\n",
    "                {\"role\": \"assistant\", \"content\": output},\n",
    "            ]\n",
    "            for input, output in zip(samples[\"input\"], samples[\"output\"])\n",
    "        ],\n",
    "        tokenize=False,\n",
    "    )\n",
    "    return [text.removeprefix(\"<bos>\") for text in chat_texts]\n",
    "\n",
    "display(create_chat_for_finetuning(train_data[:3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['gate_proj', 'down_proj', 'k_proj', 'o_proj', 'q_proj', 'v_proj', 'up_proj']\n",
      "Trainable: 216,072,192 | total: 9,457,778,176 | Percentage: 2.2846%\n"
     ]
    }
   ],
   "source": [
    "# pip install peft\n",
    "from bitsandbytes.nn import Linear4bit, Linear8bitLt\n",
    "from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model\n",
    "\n",
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "def find_all_linear_names(model):\n",
    "    lora_module_names = set()\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, Linear4bit) or isinstance(module, Linear8bitLt):\n",
    "            names = name.split('.')\n",
    "            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n",
    "        if 'lm_head' in lora_module_names: # needed for 16-bit\n",
    "            lora_module_names.remove('lm_head')\n",
    "    return list(lora_module_names)\n",
    "\n",
    "modules = find_all_linear_names(model)\n",
    "print(modules)\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=64,\n",
    "    lora_alpha=32,\n",
    "    target_modules=modules,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "trainable, total = model.get_nb_trainable_parameters()\n",
    "print(f\"Trainable: {trainable:,} | total: {total:,} | Percentage: {trainable/total*100:.4f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab0646627b2b46e18cbf94c5691e7e5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df68ab2d1dc74a58b420aae3d6a97065",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `sdpa`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [125/125 07:57, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.225300</td>\n",
       "      <td>0.119923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.121700</td>\n",
       "      <td>0.099319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.054300</td>\n",
       "      <td>0.094822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.069200</td>\n",
       "      <td>0.095361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.055700</td>\n",
       "      <td>0.087866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.084300</td>\n",
       "      <td>0.075519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.051300</td>\n",
       "      <td>0.074762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.056200</td>\n",
       "      <td>0.075127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.053400</td>\n",
       "      <td>0.073931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.079600</td>\n",
       "      <td>0.071001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.056900</td>\n",
       "      <td>0.070001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.056000</td>\n",
       "      <td>0.069649</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'global_step': 125,\n",
       " 'training_loss': 0.07978586745262146,\n",
       " 'metrics': {'train_runtime': 480.1897,\n",
       "  'train_samples_per_second': 1.041,\n",
       "  'train_steps_per_second': 0.26,\n",
       "  'total_flos': 4840080825409536.0,\n",
       "  'train_loss': 0.07978586745262146,\n",
       "  'epoch': 1.0}}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from trl import DataCollatorForCompletionOnlyLM, SFTConfig, SFTTrainer\n",
    "\n",
    "tokenizer.padding_side = \"right\"\n",
    "model.config.use_cache=False\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "collator = DataCollatorForCompletionOnlyLM(\n",
    "    instruction_template=\"<start_of_turn>user\\n\",\n",
    "    response_template=\"<start_of_turn>model\\n\",\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model,\n",
    "    args=SFTConfig(\n",
    "        output_dir=\"/tmp/finetuned_gemma_2b\",\n",
    "        per_device_train_batch_size=TRAIN_BATCH_SIZE,\n",
    "        gradient_accumulation_steps=TRAIN_GRADIENT_ACCUMULATION_STEPS,\n",
    "        gradient_checkpointing=True,\n",
    "        gradient_checkpointing_kwargs=dict(use_reentrant=False),\n",
    "        max_seq_length=TRAIN_MAX_LENGTH,\n",
    "        num_train_epochs=TRAIN_NUM_EPOCHS,\n",
    "        save_strategy=\"epoch\",\n",
    "        logging_steps=TRAIN_LOGGING_STEPS,\n",
    "        eval_steps=TRAIN_LOGGING_STEPS,\n",
    "        eval_strategy=\"steps\",\n",
    "        eval_accumulation_steps=EVAL_ACCUMULATION_STEPS,\n",
    "    ),\n",
    "    data_collator=collator,\n",
    "    eval_dataset=test_data.select(range(min(len(test_data), EVAL_SIZE))),\n",
    "    formatting_func=create_chat_for_finetuning,\n",
    "    peft_config=lora_config,\n",
    "    train_dataset=train_data,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "train_result = trainer.train()\n",
    "display(train_result._asdict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.use_cache=True\n",
    "model.gradient_checkpointing_disable()\n",
    "model.eval()\n",
    "tokenizer.padding_side = \"left\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge and evaluate the finetuned model\n",
    "\n",
    "After training the LoRA adapter has not yet been merged with the base Gemma model. This will make it run a lot slower. To merge the LoRA adapter, we will follow these steps:\n",
    "- https://huggingface.co/docs/peft/main/en/package_reference/lora#peft.LoraModel.merge_and_unload\n",
    "- https://discuss.huggingface.co/t/help-with-merging-lora-weights-back-into-base-model/40968/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7cf1a0b868646a4a871f4bd0518eb23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02bc86c12c6e4befa8f15009c5cfad93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "trainer.model.save_pretrained(\"models/lora_adapter\")\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL_ID,\n",
    "    low_cpu_mem_usage=True,\n",
    "    return_dict=True,\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "model = PeftModel.from_pretrained(base_model, \"models/lora_adapter\").merge_and_unload()\n",
    "model.save_pretrained(\"models/finetuned_model\", safe_serialization=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f5f2b7dcb10498bb32334d20a0393b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gc\n",
    "\n",
    "try: del model\n",
    "except NameError: pass\n",
    "try: del trainer\n",
    "except NameError: pass\n",
    "try: del base_model\n",
    "except NameError: pass\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"models/finetuned_model\",\n",
    "    attn_implementation='eager',\n",
    "    quantization_config=quantization_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f403192a8ca34a7d898b7312247ef52b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gemma accuracy: 54.0%\n",
      "Finetuned accuracy: 56.0%\n"
     ]
    }
   ],
   "source": [
    "batch_predict = create_predict(tokenizer, model, \"finetuned\", batch=True, generate_kwargs=GENERATE_KWARGS)\n",
    "test_data = test_data.map(batch_predict, batched=True, batch_size=EVAL_BATCH_SIZE)\n",
    "\n",
    "finetuned_accuracy = (np.asarray(test_data[\"finetuned_label\"]) == np.asarray(test_data[\"true_label\"])).mean()\n",
    "print(f\"Gemma accuracy: {round(gemma_accuracy * 100, 1)}%\")\n",
    "print(f\"Finetuned accuracy: {round(finetuned_accuracy * 100, 1)}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cloudspace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
