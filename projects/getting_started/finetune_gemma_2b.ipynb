{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_SIZE = 200\n",
    "TEST_SIZE = 20\n",
    "EVAL_SIZE = 50\n",
    "SEED = 123\n",
    "\n",
    "EVAL_BATCH_SIZE = 8\n",
    "TRAIN_BATCH_SIZE = 1\n",
    "TRAIN_GRADIENT_ACCUMULATION_STEPS = 4\n",
    "TRAIN_LOGGING_STEPS = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install datasets\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"medalpaca/medical_meadow_medqa\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing steps\n",
    "\n",
    "We're not going to focus much on the preprocessing in this tutorial, so feel free to skim over or skip this subsection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input', 'instruction', 'output'],\n",
       "        num_rows: 10178\n",
       "    })\n",
       "})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input', 'instruction', 'output'],\n",
       "        num_rows: 200\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input', 'instruction', 'output'],\n",
       "        num_rows: 20\n",
       "    })\n",
       "})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Sample train and test sets\n",
    "display(dataset)\n",
    "\n",
    "dataset = dataset[\"train\"].train_test_split(\n",
    "    train_size=TRAIN_SIZE,\n",
    "    test_size=TEST_SIZE,\n",
    "    shuffle=True,\n",
    "    seed=SEED\n",
    ")\n",
    "display(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# input\n",
      "Q:A 65-year-old man presents to the emergency department for sudden weakness. The patient states that he was at home enjoying his morning coffee when his symptoms began. He says that his left arm suddenly felt very odd and weak thus prompting him to come to the ED. The patient has a past medical history of diabetes, COPD, hypertension, anxiety, alcohol abuse, and PTSD. He recently fell off a horse while horseback riding but claims to not have experienced any significant injuries. He typically drinks 5-7 drinks per day and his last drink was yesterday afternoon. His current medications include insulin, metformin, atorvastatin, lisinopril, albuterol, and fluoxetine. His temperature is 99.5°F (37.5°C), blood pressure is 177/118 mmHg, pulse is 120/min, respirations are 18/min, and oxygen saturation is 93% on room air. On physical exam, you note an elderly man who is mildly confused. Cardiopulmonary exam demonstrates bilateral expiratory wheezes and a systolic murmur along the right upper sternal border that radiates to the carotids. Neurological exam reveals cranial nerves II-XII as grossly intact with finger-nose exam mildly abnormal on the left and heel-shin exam within normal limits. The patient has 5/5 strength in his right arm and 3/5 strength in his left arm. The patient struggles to manipulate objects such as a pen with his left hand. The patient is given a dose of diazepam and started on IV fluids. Which of the following is the most likely diagnosis in this patient?? \n",
      "{'A': 'Berry aneurysm rupture', 'B': 'Bridging vein tear', 'C': 'Cerebellar bleeding', 'D': 'Hypertensive encephalopathy', 'E': 'Lipohyalinosis'},\n",
      "\n",
      "# instruction\n",
      "Please answer with one of the option in the bracket\n",
      "\n",
      "# output\n",
      "E: Lipohyalinosis\n"
     ]
    }
   ],
   "source": [
    "# original format of the dataset\n",
    "def print_sample(sample: dict[str, str]):\n",
    "    message = \"\\n\".join(\n",
    "        f\"\\n# {k}\\n{v}\" for k, v in sample.items()\n",
    "    )[1:]\n",
    "    print(message)\n",
    "\n",
    "print_sample(example := dataset[\"train\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input', 'output', 'true_label'],\n",
       "        num_rows: 200\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input', 'output', 'true_label'],\n",
       "        num_rows: 20\n",
       "    })\n",
       "})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# input\n",
      "Q: A 65-year-old man presents to the emergency department for sudden weakness. The patient states that he was at home enjoying his morning coffee when his symptoms began. He says that his left arm suddenly felt very odd and weak thus prompting him to come to the ED. The patient has a past medical history of diabetes, COPD, hypertension, anxiety, alcohol abuse, and PTSD. He recently fell off a horse while horseback riding but claims to not have experienced any significant injuries. He typically drinks 5-7 drinks per day and his last drink was yesterday afternoon. His current medications include insulin, metformin, atorvastatin, lisinopril, albuterol, and fluoxetine. His temperature is 99.5°F (37.5°C), blood pressure is 177/118 mmHg, pulse is 120/min, respirations are 18/min, and oxygen saturation is 93% on room air. On physical exam, you note an elderly man who is mildly confused. Cardiopulmonary exam demonstrates bilateral expiratory wheezes and a systolic murmur along the right upper sternal border that radiates to the carotids. Neurological exam reveals cranial nerves II-XII as grossly intact with finger-nose exam mildly abnormal on the left and heel-shin exam within normal limits. The patient has 5/5 strength in his right arm and 3/5 strength in his left arm. The patient struggles to manipulate objects such as a pen with his left hand. The patient is given a dose of diazepam and started on IV fluids. Which of the following is the most likely diagnosis in this patient?? Give your answer as a JSON dictionary in the form of {\"option\": \"A-E\", \"text\": \"corresponding text\"}. No yapping.\n",
      "{'A': 'Berry aneurysm rupture', 'B': 'Bridging vein tear', 'C': 'Cerebellar bleeding', 'D': 'Hypertensive encephalopathy', 'E': 'Lipohyalinosis'}\n",
      "\n",
      "# output\n",
      "{\"option\": \"E\", \"text\": \"Lipohyalinosis\"}\n",
      "\n",
      "# true_label\n",
      "E\n"
     ]
    }
   ],
   "source": [
    "# reformat the dataset\n",
    "import json\n",
    "\n",
    "def reformat_sample(sample: dict[str, str]) -> dict[str, str]:\n",
    "    input = \"Q: \" + sample[\"input\"].removeprefix(\"Q:\").removesuffix(\",\")\n",
    "    input = input.replace(\n",
    "        \"\\n{\",\n",
    "        (\n",
    "            'Give your answer as a JSON dictionary in the form of'\n",
    "            ' {\"option\": \"A-E\", \"text\": \"corresponding text\"}.'\n",
    "            ' No yapping.'\n",
    "            '\\n{'\n",
    "        )\n",
    "    )\n",
    "    answer_option = sample[\"output\"][0]\n",
    "    answer_text = sample[\"output\"][3:]\n",
    "    true_label = answer_option\n",
    "    output = json.dumps({\"option\": answer_option, \"text\": answer_text})\n",
    "    return {\"input\": input, \"output\": output, \"true_label\": true_label}\n",
    "\n",
    "\n",
    "dataset = dataset.map(reformat_sample).remove_columns(\"instruction\")\n",
    "\n",
    "display(dataset)\n",
    "print_sample(example := dataset[\"train\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finetuning dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Gemma 2B instruct model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n",
      "`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n",
      "Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n",
      "`config.hidden_activation` if you want to override this behaviour.\n",
      "See https://github.com/huggingface/transformers/pull/29402 for more details.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff95b464dcab46ad995c2018f412142c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# pip install torch transformers\n",
    "# pip install bitsandbytes accelerate\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b-it\")\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(load_in_4bit=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"google/gemma-2b-it\",\n",
    "    quantization_config=quantization_config\n",
    ")\n",
    "print(model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/bitsandbytes/nn/modules.py:426: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos><start_of_turn>user\n",
      "Q: A 65-year-old man presents to the emergency department for sudden weakness. The patient states that he was at home enjoying his morning coffee when his symptoms began. He says that his left arm suddenly felt very odd and weak thus prompting him to come to the ED. The patient has a past medical history of diabetes, COPD, hypertension, anxiety, alcohol abuse, and PTSD. He recently fell off a horse while horseback riding but claims to not have experienced any significant injuries. He typically drinks 5-7 drinks per day and his last drink was yesterday afternoon. His current medications include insulin, metformin, atorvastatin, lisinopril, albuterol, and fluoxetine. His temperature is 99.5°F (37.5°C), blood pressure is 177/118 mmHg, pulse is 120/min, respirations are 18/min, and oxygen saturation is 93% on room air. On physical exam, you note an elderly man who is mildly confused. Cardiopulmonary exam demonstrates bilateral expiratory wheezes and a systolic murmur along the right upper sternal border that radiates to the carotids. Neurological exam reveals cranial nerves II-XII as grossly intact with finger-nose exam mildly abnormal on the left and heel-shin exam within normal limits. The patient has 5/5 strength in his right arm and 3/5 strength in his left arm. The patient struggles to manipulate objects such as a pen with his left hand. The patient is given a dose of diazepam and started on IV fluids. Which of the following is the most likely diagnosis in this patient?? Give your answer as a JSON dictionary in the form of {\"option\": \"A-E\", \"text\": \"corresponding text\"}. No yapping.\n",
      "{'A': 'Berry aneurysm rupture', 'B': 'Bridging vein tear', 'C': 'Cerebellar bleeding', 'D': 'Hypertensive encephalopathy', 'E': 'Lipohyalinosis'}<end_of_turn>\n",
      "<start_of_turn>model\n",
      "{\"option\": \"A-Berry aneurysm rupture\", \"text\": \"A berry aneurysm rupture is the most likely diagnosis in this patient due to the patient's history of trauma, recent fall, and symptoms of a ruptured artery.\"}<eos>\n"
     ]
    }
   ],
   "source": [
    "chat = [\n",
    "    {\"role\": \"user\", \"content\": example[\"input\"]},\n",
    "]\n",
    "input_ids = tokenizer.apply_chat_template(chat, add_generation_prompt=True, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "output_ids = model.generate(\n",
    "    input_ids,\n",
    "    do_sample=True,\n",
    "    max_new_tokens=512,\n",
    "    temperature=1e-3,\n",
    ")\n",
    "\n",
    "print(tokenizer.decode(output_ids[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"option\": \"A-Berry aneurysm rupture\", \"text\": \"A berry aneurysm rupture is the most likely diagnosis in this patient due to the patient's history of trauma, recent fall, and symptoms of a ruptured artery.\"}\n"
     ]
    }
   ],
   "source": [
    "def get_generated_texts(input_ids: torch.Tensor, output_ids: torch.Tensor, remove_eos: bool = True) -> list[str]:\n",
    "    \"\"\"Retreive only the generated texts based on input and output ids\n",
    "\n",
    "    Args:\n",
    "        input_ids (torch.Tensor): batch of input ids.\n",
    "        output_ids (torch.Tensor): corresponding output ids.\n",
    "        remove_eos (bool, optional): whether to remove the final <eos> token. Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "        list[str]: the batch of text generated by the model based on the input texts.\n",
    "    \"\"\"\n",
    "    texts = [\n",
    "        tokenizer.decode(out_seq[len(in_seq):])\n",
    "        for in_seq, out_seq in zip(input_ids, output_ids)\n",
    "    ]\n",
    "    if remove_eos:\n",
    "        texts = [text.removesuffix(\"<eos>\") for text in texts]\n",
    "    return texts\n",
    "\n",
    "print(get_generated_texts(input_ids, output_ids)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Gemma on test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation utils\n",
    "\n",
    "These utils are used to extract the final label from the generated text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'A': 'Early disseminated Lyme disease',\n",
       " 'B': 'Embolic stroke at the posterior inferior cerebellar artery (PICA)',\n",
       " 'C': 'Hypoperfusion of the anterior spinal artery (ASA)',\n",
       " 'D': 'Labryrinthitis',\n",
       " 'E': 'Thrombotic stroke at the anterior inferior cerebellar artery (AICA)'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'E'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# pip install rapidfuzz\n",
    "from rapidfuzz import fuzz\n",
    "from rapidfuzz.utils import default_process\n",
    "\n",
    "example_passage = \"\"\"\n",
    "Q:A 67-year-old man with a past medical history of poorly-controlled type 2 diabetes mellitus (T2DM) is brought to the emergency department for acute onset nausea and vomiting. According to the patient, he suddenly experienced vertigo and began vomiting 3 hours ago while watching TV. He reports hiking in New Hampshire with his wife 2 days ago. Past medical history is significant for a myocardial infarction (MI) that was treated with cardiac stenting, T2DM, and hypertension. Medications include lisinopril, aspirin, atorvastatin, warfarin, and insulin. Physical examination demonstrates left-sided facial droop and decreased pinprick sensation at the right arm and leg. What is the most likely etiology of this patient’s symptoms?? {'A': 'Early disseminated Lyme disease', 'B': 'Embolic stroke at the posterior inferior cerebellar artery (PICA)', 'C': 'Hypoperfusion of the anterior spinal artery (ASA)', 'D': 'Labryrinthitis', 'E': 'Thrombotic stroke at the anterior inferior cerebellar artery (AICA)'},\n",
    "\"\"\".strip()\n",
    "\n",
    "example_substr = \"stroke at the anterior inferior cerebellar artery\"\n",
    "\n",
    "def get_available_qa_choices(passage: str) -> dict[str, str]:\n",
    "    return {\n",
    "        match.group(1): match.group(2)\n",
    "        for match in re.finditer(\"'([^']+)': '([^']+)'\", passage)\n",
    "    }\n",
    "\n",
    "def get_best_match(answer_text: str, passage: str) -> str:\n",
    "    choices = get_available_qa_choices(passage)\n",
    "    if not choices:\n",
    "        return \"\"  # no choices found\n",
    "    return sorted(\n",
    "        choices,\n",
    "        key=lambda c: fuzz.token_set_ratio(choices[c], answer_text, processor=default_process)\n",
    "    )[-1]\n",
    "\n",
    "display(get_available_qa_choices(example_passage))\n",
    "display(get_best_match(example_substr, example_passage))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'E'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'A'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "example_pred_text = \"\"\"\n",
    "{\n",
    "\"option\": \"A\",\n",
    "\"answer\": \"Early disseminated Lyme disease\"\n",
    "}<eos>\n",
    "\"\"\".strip()\n",
    "\n",
    "def parse_prediction(pred_text: str, passage: str = \"\") -> str:\n",
    "    \"\"\"Parse the predicted answer based on the output text, with text matching as backup.\n",
    "\n",
    "    Args:\n",
    "        pred_text (str): text outputted from the language model.\n",
    "        passage (str, optional): The input passage for text matchingin case the LLM does\n",
    "            not output parseable JSON. Useful for evaluating the model before finetuning.\n",
    "            Defaults to \"\" (no passage); in this case the backup prediction will be \"\".\n",
    "\n",
    "    Returns:\n",
    "        str: option (a letter or \"\") predicted by the model.\n",
    "    \"\"\"\n",
    "    json_text = pred_text\n",
    "    if match := re.search(r\"\\{\", json_text):  # remove anything before first {\n",
    "        json_text = json_text[match.start() :]\n",
    "    if match := re.search(r\"\\}\", json_text[::-1]):  # remove anything after last }\n",
    "        json_text = json_text[: len(json_text) - match.start()]\n",
    "\n",
    "    try:\n",
    "        if match := re.match(r\"^[a-eA-E]$\", json.loads(json_text)[\"option\"].strip()):\n",
    "            return match.group(0)\n",
    "    except (json.JSONDecodeError, KeyError):\n",
    "        pass\n",
    "\n",
    "    # backup: if a passage is supplied, get the best match\n",
    "    if passage:\n",
    "        return get_best_match(pred_text, passage)\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "display(parse_prediction(example_pred_text))\n",
    "display(parse_prediction(example_substr, example_passage))\n",
    "display(parse_prediction('{\"option\": \"A - Early disseminated Lyme disease\"}', example_passage))\n",
    "display(parse_prediction(example_substr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37358700448a4c20bdbf7058a0614dcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/20 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gemma accuracy: 20.0%\n"
     ]
    }
   ],
   "source": [
    "from typing import Callable\n",
    "from transformers.modeling_utils import PreTrainedModel\n",
    "\n",
    "def batch_get_preds(model: PreTrainedModel, model_name: str) -> Callable[[dict], dict]:\n",
    "    def _get_preds(samples: dict) -> dict:\n",
    "        input_ids = tokenizer.apply_chat_template(\n",
    "            [[{\"role\": \"user\", \"content\": text}] for text in samples[\"input\"]],\n",
    "            add_generation_prompt=True,\n",
    "            padding=True,\n",
    "            return_tensors=\"pt\",\n",
    "        ).to(model.device)\n",
    "\n",
    "        output_ids = model.generate(\n",
    "            input_ids,\n",
    "            do_sample=True,\n",
    "            max_new_tokens=512,\n",
    "            temperature=1e-3,\n",
    "        )\n",
    "\n",
    "        pred_texts = get_generated_texts(input_ids, output_ids)\n",
    "        samples[f\"{model_name}_pred\"] = pred_texts\n",
    "        samples[f\"{model_name}_label\"] = pred_labels = [\n",
    "            parse_prediction(pred, text)\n",
    "            for text, pred in zip(samples[\"input\"], pred_texts)\n",
    "        ]\n",
    "        samples[f\"{model_name}_correct\"] = [\n",
    "            pred_label == true_label\n",
    "            for true_label, pred_label in zip(samples[\"true_label\"], pred_labels)\n",
    "        ]\n",
    "\n",
    "        return samples\n",
    "    return _get_preds\n",
    "\n",
    "test_set = dataset[\"test\"]\n",
    "test_set = test_set.map(batch_get_preds(model, \"gemma\"), batched=True, batch_size=EVAL_BATCH_SIZE)\n",
    "\n",
    "gemma_accuracy = sum(test_set[\"gemma_correct\"]) / len(test_set)\n",
    "print(f\"Gemma accuracy: {round(gemma_accuracy * 100, 1)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<start_of_turn>user\\nQ: A 65-year-old man presents to the emergency department for sudden weakness. The patient states that he was at home enjoying his morning coffee when his symptoms began. He says that his left arm suddenly felt very odd and weak thus prompting him to come to the ED. The patient has a past medical history of diabetes, COPD, hypertension, anxiety, alcohol abuse, and PTSD. He recently fell off a horse while horseback riding but claims to not have experienced any significant injuries. He typically drinks 5-7 drinks per day and his last drink was yesterday afternoon. His current medications include insulin, metformin, atorvastatin, lisinopril, albuterol, and fluoxetine. His temperature is 99.5°F (37.5°C), blood pressure is 177/118 mmHg, pulse is 120/min, respirations are 18/min, and oxygen saturation is 93% on room air. On physical exam, you note an elderly man who is mildly confused. Cardiopulmonary exam demonstrates bilateral expiratory wheezes and a systolic murmur along the right upper sternal border that radiates to the carotids. Neurological exam reveals cranial nerves II-XII as grossly intact with finger-nose exam mildly abnormal on the left and heel-shin exam within normal limits. The patient has 5/5 strength in his right arm and 3/5 strength in his left arm. The patient struggles to manipulate objects such as a pen with his left hand. The patient is given a dose of diazepam and started on IV fluids. Which of the following is the most likely diagnosis in this patient?? Give your answer as a JSON dictionary in the form of {\"option\": \"A-E\", \"text\": \"corresponding text\"}. No yapping.\\n{\\'A\\': \\'Berry aneurysm rupture\\', \\'B\\': \\'Bridging vein tear\\', \\'C\\': \\'Cerebellar bleeding\\', \\'D\\': \\'Hypertensive encephalopathy\\', \\'E\\': \\'Lipohyalinosis\\'}<end_of_turn>\\n<start_of_turn>model\\n{\"option\": \"E\", \"text\": \"Lipohyalinosis\"}<end_of_turn>\\n',\n",
       " '<start_of_turn>user\\nQ: A 68-year-old male smoker dies suddenly in a car accident. He had smoked 2 packs per day for 40 years. His past medical history is notable for a frequent, very productive cough, recurrent respiratory infections and occasional wheezing. He had no other medical problems. At autopsy, which of the following is most likely to be found in this patient?? Give your answer as a JSON dictionary in the form of {\"option\": \"A-E\", \"text\": \"corresponding text\"}. No yapping.\\n{\\'A\\': \\'Interstitial fibrosis of the lung\\', \\'B\\': \\'Pleural plaques\\', \\'C\\': \\'Increased number and activity of goblet cells\\', \\'D\\': \\'Ferruginous bodies\\', \\'E\\': \\'Mucous gland atrophy\\'}<end_of_turn>\\n<start_of_turn>model\\n{\"option\": \"C\", \"text\": \"Increased number and activity of goblet cells\"}<end_of_turn>\\n']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_set = dataset[\"train\"]\n",
    "\n",
    "def create_chat_for_finetuning(samples: dict) -> list[str]:\n",
    "    chat_texts = tokenizer.apply_chat_template(\n",
    "        [\n",
    "            [\n",
    "                {\"role\": \"user\", \"content\": input},\n",
    "                {\"role\": \"assistant\", \"content\": output},\n",
    "            ]\n",
    "            for input, output in zip(samples[\"input\"], samples[\"output\"])\n",
    "        ],\n",
    "        tokenize=False,\n",
    "    )\n",
    "    return [text.removeprefix(\"<bos>\") for text in chat_texts]\n",
    "\n",
    "display(create_chat_for_finetuning(dataset[\"train\"][:2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['v_proj', 'gate_proj', 'k_proj', 'q_proj', 'down_proj', 'up_proj', 'o_proj']\n",
      "Trainable: 78,446,592 | total: 2,584,619,008 | Percentage: 3.0351%\n"
     ]
    }
   ],
   "source": [
    "# pip install peft\n",
    "from bitsandbytes.nn import Linear4bit, Linear8bitLt\n",
    "from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model\n",
    "\n",
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "def find_all_linear_names(model):\n",
    "    lora_module_names = set()\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, Linear4bit) or isinstance(module, Linear8bitLt):\n",
    "            names = name.split('.')\n",
    "            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n",
    "        if 'lm_head' in lora_module_names: # needed for 16-bit\n",
    "            lora_module_names.remove('lm_head')\n",
    "    return list(lora_module_names)\n",
    "\n",
    "modules = find_all_linear_names(model)\n",
    "print(modules)\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=64,\n",
    "    lora_alpha=32,\n",
    "    target_modules=modules,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "trainable, total = model.get_nb_trainable_parameters()\n",
    "print(f\"Trainable: {trainable:,} | total: {total:,} | Percentage: {trainable/total*100:.4f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [18/50 01:02 < 02:04, 0.26 it/s, Epoch 0.34/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.055900</td>\n",
       "      <td>0.214258</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 38\u001b[0m\n\u001b[1;32m      8\u001b[0m collator \u001b[38;5;241m=\u001b[39m DataCollatorForCompletionOnlyLM(\n\u001b[1;32m      9\u001b[0m     instruction_template\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<start_of_turn>user\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     10\u001b[0m     response_template\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<start_of_turn>model\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     11\u001b[0m     tokenizer\u001b[38;5;241m=\u001b[39mtokenizer,\n\u001b[1;32m     12\u001b[0m     mlm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     13\u001b[0m )\n\u001b[1;32m     15\u001b[0m trainer \u001b[38;5;241m=\u001b[39m SFTTrainer(\n\u001b[1;32m     16\u001b[0m     model,\n\u001b[1;32m     17\u001b[0m     args\u001b[38;5;241m=\u001b[39mSFTConfig(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     35\u001b[0m     tokenizer\u001b[38;5;241m=\u001b[39mtokenizer,\n\u001b[1;32m     36\u001b[0m )\n\u001b[0;32m---> 38\u001b[0m train_result \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m display(train_result\u001b[38;5;241m.\u001b[39m_asdict())\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:440\u001b[0m, in \u001b[0;36mSFTTrainer.train\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    437\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneftune_noise_alpha \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trainer_supports_neftune:\n\u001b[1;32m    438\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trl_activate_neftune(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel)\n\u001b[0;32m--> 440\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    442\u001b[0m \u001b[38;5;66;03m# After training we make sure to retrieve back the original forward pass method\u001b[39;00m\n\u001b[1;32m    443\u001b[0m \u001b[38;5;66;03m# for the embedding layer by removing the forward post hook.\u001b[39;00m\n\u001b[1;32m    444\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneftune_noise_alpha \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trainer_supports_neftune:\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/trainer.py:1932\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1930\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1931\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1932\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1933\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1934\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1935\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1936\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1937\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/trainer.py:2268\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2265\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   2267\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 2268\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2270\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2271\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2272\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2273\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2274\u001b[0m ):\n\u001b[1;32m   2275\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2276\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/trainer.py:3324\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   3322\u001b[0m         scaled_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m   3323\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3324\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3326\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach() \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/accelerate/accelerator.py:2134\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2132\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlomo_backward(loss, learning_rate)\n\u001b[1;32m   2133\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2134\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# pip install trl\n",
    "from trl import DataCollatorForCompletionOnlyLM, SFTConfig, SFTTrainer\n",
    "\n",
    "tokenizer.padding_side = \"right\"\n",
    "model.config.use_cache=False\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "collator = DataCollatorForCompletionOnlyLM(\n",
    "    instruction_template=\"<start_of_turn>user\\n\",\n",
    "    response_template=\"<start_of_turn>model\\n\",\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model,\n",
    "    args=SFTConfig(\n",
    "        output_dir=\"/tmp/finetuned_gemma_2b\",\n",
    "        per_device_train_batch_size=TRAIN_BATCH_SIZE,\n",
    "        gradient_accumulation_steps=TRAIN_GRADIENT_ACCUMULATION_STEPS,\n",
    "        gradient_checkpointing=True,\n",
    "        gradient_checkpointing_kwargs=dict(use_reentrant=False),\n",
    "        max_seq_length=2048,\n",
    "        num_train_epochs=1,\n",
    "        save_strategy=\"epoch\",\n",
    "        logging_steps=TRAIN_LOGGING_STEPS,\n",
    "        eval_steps=TRAIN_LOGGING_STEPS,\n",
    "        eval_strategy=\"steps\",\n",
    "    ),\n",
    "    data_collator=collator,\n",
    "    eval_dataset=test_set.select(range(min(len(test_set), EVAL_SIZE))),\n",
    "    formatting_func=create_chat_for_finetuning,\n",
    "    peft_config=lora_config,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "train_result = trainer.train()\n",
    "display(train_result._asdict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.padding_side = \"left\"\n",
    "model.config.use_cache=True\n",
    "model.gradient_checkpointing_disable()\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e8039b021ad43d98d900d4f89ebf280",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/20 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gemma accuracy: 20.0%\n",
      "Finetuned accuracy: 20.0%\n"
     ]
    }
   ],
   "source": [
    "test_set = test_set.map(batch_get_preds(model, \"finetuned\"), batched=True, batch_size=EVAL_BATCH_SIZE)\n",
    "finetuned_accuracy = sum(test_set[\"finetuned_correct\"]) / len(test_set)\n",
    "\n",
    "print(f\"Gemma accuracy: {round(gemma_accuracy * 100, 1)}%\")\n",
    "print(f\"Finetuned accuracy: {round(finetuned_accuracy * 100, 1)}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cloudspace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
