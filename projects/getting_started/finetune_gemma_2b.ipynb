{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n",
      "`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n",
      "Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n",
      "`config.hidden_activation` if you want to override this behaviour.\n",
      "See https://github.com/huggingface/transformers/pull/29402 for more details.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "683784c59605416c9ef6306a93fbeef7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# pip install torch transformers\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b-it\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    # pip install bitsandbytes accelerate\n",
    "    from transformers import BitsAndBytesConfig\n",
    "\n",
    "    device = \"cuda\"\n",
    "    quantization_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        \"google/gemma-2b-it\",\n",
    "        quantization_config=quantization_config\n",
    "    )\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        \"google/gemma-2b-it\",\n",
    "        torch_dtype=torch.bfloat16\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos>Q:An 82-year-old male with congestive heart failure experiences rapid decompensation of his condition, manifesting as worsening dyspnea, edema, and increased fatigue. Labs reveal an increase in his serum creatinine from baseline. As part of the management of this acute change, the patient is given IV dobutamine to alleviate his symptoms. Which of the following effects occur as a result of this therapy?? {'A': 'Slowed atrioventricular conduction velocities', 'B': 'Increased myocardial oxygen consumption', 'C': 'Decreased heart rate', 'D': 'Increased systemic vascular resistance due to systemic vasoconstriction', 'E': 'Decreased cardiac contractility'},\n",
      "\n",
      "Provide your answer as a JSON dictionary with the \"option\" and answer \"text\".\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"option\": \"B\",\n",
      "  \"answer\": \"Increased myocardial oxygen consumption\"\n",
      "}\n",
      "```<eos>\n"
     ]
    }
   ],
   "source": [
    "input_text = \"\"\"\n",
    "Q:An 82-year-old male with congestive heart failure experiences rapid decompensation of his condition, manifesting as worsening dyspnea, edema, and increased fatigue. Labs reveal an increase in his serum creatinine from baseline. As part of the management of this acute change, the patient is given IV dobutamine to alleviate his symptoms. Which of the following effects occur as a result of this therapy?? {'A': 'Slowed atrioventricular conduction velocities', 'B': 'Increased myocardial oxygen consumption', 'C': 'Decreased heart rate', 'D': 'Increased systemic vascular resistance due to systemic vasoconstriction', 'E': 'Decreased cardiac contractility'},\n",
    "\n",
    "Provide your answer as a JSON dictionary with the \"option\" and answer \"text\".\n",
    "\"\"\".strip()\n",
    "\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
    "outputs = model.generate(\n",
    "    **input_ids,\n",
    "    do_sample=True,\n",
    "    max_new_tokens=512,\n",
    "    temperature=1e-3,\n",
    ")\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cloudspace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
