{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_SIZE = 30\n",
    "TEST_SIZE = 3\n",
    "SEED = 123\n",
    "BATCH_SIZE = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install datasets\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"medalpaca/medical_meadow_medqa\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing steps\n",
    "\n",
    "We're not going to focus much on the preprocessing in this tutorial, so feel free to skim over or skip this subsection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input', 'instruction', 'output'],\n",
       "        num_rows: 10178\n",
       "    })\n",
       "})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input', 'instruction', 'output'],\n",
       "        num_rows: 30\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input', 'instruction', 'output'],\n",
       "        num_rows: 3\n",
       "    })\n",
       "})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Sample train and test sets\n",
    "display(dataset)\n",
    "\n",
    "dataset = dataset[\"train\"].train_test_split(\n",
    "    train_size=TRAIN_SIZE,\n",
    "    test_size=TEST_SIZE,\n",
    "    shuffle=True,\n",
    "    seed=SEED\n",
    ")\n",
    "display(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# input\n",
      "Q:A 9-year-old girl is brought to the pediatrician by her parents because of unremitting cough, fevers, night sweats, anorexia, and weight loss for 4 weeks. Her vaccinations are up to date. When asked about recent exposure to an ill person, the parents mention that she is frequently under the care of a middle-aged woman who recently immigrated from a small rural community in north India. Her temperature is 39.0°C (102.2°F), respiratory rate is 30/min, and heart rate is 120/min. Her weight is 2 standard deviations below normal for her age. Chest auscultation shows fine crackles in both lung fields.  The patient is referred to a nearby children’s hospital where her clinical condition rapidly worsens over several weeks. A chest radiograph is shown. Microbiological evaluation of a bronchial aspirate reveals an organism with a cell wall that is impervious to Gram stain. Which of the following best describes the cell wall of the causative agent?? \n",
      "{'A': 'Low muramic acid content', 'B': 'High mycolic acid content', 'C': 'High ergosterol content', 'D': 'Absence of cellular wall', 'E': 'Teichoic acid-rich cellular wall'},\n",
      "\n",
      "# instruction\n",
      "Please answer with one of the option in the bracket\n",
      "\n",
      "# output\n",
      "B: High mycolic acid content\n"
     ]
    }
   ],
   "source": [
    "# original format of the dataset\n",
    "def print_sample(sample: dict[str, str]):\n",
    "    message = \"\\n\".join(\n",
    "        f\"\\n# {k}\\n{v}\" for k, v in sample.items()\n",
    "    )[1:]\n",
    "    print(message)\n",
    "\n",
    "print_sample(example := dataset[\"train\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input', 'output', 'true_label'],\n",
       "        num_rows: 30\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input', 'output', 'true_label'],\n",
       "        num_rows: 3\n",
       "    })\n",
       "})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# input\n",
      "Q: A 9-year-old girl is brought to the pediatrician by her parents because of unremitting cough, fevers, night sweats, anorexia, and weight loss for 4 weeks. Her vaccinations are up to date. When asked about recent exposure to an ill person, the parents mention that she is frequently under the care of a middle-aged woman who recently immigrated from a small rural community in north India. Her temperature is 39.0°C (102.2°F), respiratory rate is 30/min, and heart rate is 120/min. Her weight is 2 standard deviations below normal for her age. Chest auscultation shows fine crackles in both lung fields.  The patient is referred to a nearby children’s hospital where her clinical condition rapidly worsens over several weeks. A chest radiograph is shown. Microbiological evaluation of a bronchial aspirate reveals an organism with a cell wall that is impervious to Gram stain. Which of the following best describes the cell wall of the causative agent?? Give your answer as a JSON dictionary in the form of {\"option\": \"A-E\", \"text\": \"corresponding text\"}. No yapping.\n",
      "{'A': 'Low muramic acid content', 'B': 'High mycolic acid content', 'C': 'High ergosterol content', 'D': 'Absence of cellular wall', 'E': 'Teichoic acid-rich cellular wall'}\n",
      "\n",
      "# output\n",
      "{\"option\": \"B\", \"text\": \"High mycolic acid content\"}\n",
      "\n",
      "# true_label\n",
      "B\n"
     ]
    }
   ],
   "source": [
    "# reformat the dataset\n",
    "import json\n",
    "\n",
    "def reformat_sample(sample: dict[str, str]) -> dict[str, str]:\n",
    "    input = \"Q: \" + sample[\"input\"].removeprefix(\"Q:\").removesuffix(\",\")\n",
    "    input = input.replace(\n",
    "        \"\\n{\",\n",
    "        (\n",
    "            'Give your answer as a JSON dictionary in the form of'\n",
    "            ' {\"option\": \"A-E\", \"text\": \"corresponding text\"}.'\n",
    "            ' No yapping.'\n",
    "            '\\n{'\n",
    "        )\n",
    "    )\n",
    "\n",
    "    answer_option = sample[\"output\"][0]\n",
    "    answer_text = sample[\"output\"][3:]\n",
    "    true_label = answer_option\n",
    "    output = json.dumps({\"option\": answer_option, \"text\": answer_text})\n",
    "\n",
    "    return {\"input\": input, \"output\": output, \"true_label\": true_label}\n",
    "\n",
    "\n",
    "dataset = dataset.map(reformat_sample).remove_columns(\"instruction\")\n",
    "\n",
    "display(dataset)\n",
    "print_sample(example := dataset[\"train\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finetuning dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Gemma 2B instruct model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n",
      "Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n",
      "`config.hidden_activation` if you want to override this behaviour.\n",
      "See https://github.com/huggingface/transformers/pull/29402 for more details.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "781a7ef08c1542ecb88da7c2d8be6e61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# pip install torch transformers\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b-it\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    # pip install bitsandbytes accelerate\n",
    "    from transformers import BitsAndBytesConfig\n",
    "\n",
    "    quantization_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "else:\n",
    "    quantization_config = None\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"google/gemma-2b-it\",\n",
    "    quantization_config=quantization_config\n",
    ")\n",
    "print(model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos><start_of_turn>user\n",
      "Q: A 9-year-old girl is brought to the pediatrician by her parents because of unremitting cough, fevers, night sweats, anorexia, and weight loss for 4 weeks. Her vaccinations are up to date. When asked about recent exposure to an ill person, the parents mention that she is frequently under the care of a middle-aged woman who recently immigrated from a small rural community in north India. Her temperature is 39.0°C (102.2°F), respiratory rate is 30/min, and heart rate is 120/min. Her weight is 2 standard deviations below normal for her age. Chest auscultation shows fine crackles in both lung fields.  The patient is referred to a nearby children’s hospital where her clinical condition rapidly worsens over several weeks. A chest radiograph is shown. Microbiological evaluation of a bronchial aspirate reveals an organism with a cell wall that is impervious to Gram stain. Which of the following best describes the cell wall of the causative agent?? Give your answer as a JSON dictionary in the form of {\"option\": \"A-E\", \"text\": \"corresponding text\"}. No yapping.\n",
      "{'A': 'Low muramic acid content', 'B': 'High mycolic acid content', 'C': 'High ergosterol content', 'D': 'Absence of cellular wall', 'E': 'Teichoic acid-rich cellular wall'}<end_of_turn>\n",
      "<start_of_turn>model\n",
      "{\"option\": \"D\", \"text\": \"Absence of cellular wall\"}<eos>\n"
     ]
    }
   ],
   "source": [
    "chat = [\n",
    "    {\"role\": \"user\", \"content\": example[\"input\"]},\n",
    "]\n",
    "input_ids = tokenizer.apply_chat_template(chat, add_generation_prompt=True, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "output_ids = model.generate(\n",
    "    input_ids,\n",
    "    do_sample=True,\n",
    "    max_new_tokens=512,\n",
    "    temperature=1e-3,\n",
    ")\n",
    "\n",
    "print(tokenizer.decode(output_ids[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"option\": \"D\", \"text\": \"Absence of cellular wall\"}\n"
     ]
    }
   ],
   "source": [
    "def get_generated_texts(input_ids: torch.Tensor, output_ids: torch.Tensor, remove_eos: bool = True) -> list[str]:\n",
    "    \"\"\"Retreive only the generated text based on input and output ids\n",
    "\n",
    "    Args:\n",
    "        input_ids (torch.Tensor): batch of input ids\n",
    "        output_ids (torch.Tensor): corresponding output ids\n",
    "        remove_eos (bool, optional): whether to remove the final <eos> token. Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "        list[str]: _description_\n",
    "    \"\"\"\n",
    "    texts = [\n",
    "        tokenizer.decode(out_seq[len(in_seq):])\n",
    "        for in_seq, out_seq in zip(input_ids, output_ids)\n",
    "    ]\n",
    "    if remove_eos:\n",
    "        texts = [text.removesuffix(\"<eos>\") for text in texts]\n",
    "    return texts\n",
    "\n",
    "print(get_generated_texts(input_ids, output_ids)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate before finetuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation utils\n",
    "\n",
    "These utils are used to extract the final label from the generated text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'A': 'Early disseminated Lyme disease',\n",
       " 'B': 'Embolic stroke at the posterior inferior cerebellar artery (PICA)',\n",
       " 'C': 'Hypoperfusion of the anterior spinal artery (ASA)',\n",
       " 'D': 'Labryrinthitis',\n",
       " 'E': 'Thrombotic stroke at the anterior inferior cerebellar artery (AICA)'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'E'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# pip install rapidfuzz\n",
    "from rapidfuzz import fuzz\n",
    "from rapidfuzz.utils import default_process\n",
    "\n",
    "example_passage = \"\"\"\n",
    "Q:A 67-year-old man with a past medical history of poorly-controlled type 2 diabetes mellitus (T2DM) is brought to the emergency department for acute onset nausea and vomiting. According to the patient, he suddenly experienced vertigo and began vomiting 3 hours ago while watching TV. He reports hiking in New Hampshire with his wife 2 days ago. Past medical history is significant for a myocardial infarction (MI) that was treated with cardiac stenting, T2DM, and hypertension. Medications include lisinopril, aspirin, atorvastatin, warfarin, and insulin. Physical examination demonstrates left-sided facial droop and decreased pinprick sensation at the right arm and leg. What is the most likely etiology of this patient’s symptoms?? {'A': 'Early disseminated Lyme disease', 'B': 'Embolic stroke at the posterior inferior cerebellar artery (PICA)', 'C': 'Hypoperfusion of the anterior spinal artery (ASA)', 'D': 'Labryrinthitis', 'E': 'Thrombotic stroke at the anterior inferior cerebellar artery (AICA)'},\n",
    "\"\"\".strip()\n",
    "\n",
    "example_substr = \"stroke at the anterior inferior cerebellar artery\"\n",
    "\n",
    "def get_available_qa_choices(passage: str) -> dict[str, str]:\n",
    "    return {\n",
    "        match.group(1): match.group(2)\n",
    "        for match in re.finditer(\"'([^']+)': '([^']+)'\", passage)\n",
    "    }\n",
    "\n",
    "def get_best_match(answer_text: str, passage: str) -> str:\n",
    "    choices = get_available_qa_choices(passage)\n",
    "    if not choices:\n",
    "        return \"\"  # no choices found\n",
    "    return sorted(\n",
    "        choices,\n",
    "        key=lambda c: fuzz.token_set_ratio(choices[c], answer_text, processor=default_process)\n",
    "    )[-1]\n",
    "\n",
    "display(get_available_qa_choices(example_passage))\n",
    "display(get_best_match(example_substr, example_passage))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'E'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'A'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "example_pred_text = \"\"\"\n",
    "{\n",
    "\"option\": \"A\",\n",
    "\"answer\": \"Early disseminated Lyme disease\"\n",
    "}<eos>\n",
    "\"\"\".strip()\n",
    "\n",
    "def parse_prediction(pred_text: str, passage: str = \"\") -> str:\n",
    "    \"\"\"Parse the predicted answer based on the output text, with text matching as backup.\n",
    "\n",
    "    Args:\n",
    "        pred_text (str): text outputted from the language model.\n",
    "        passage (str, optional): The input passage for text matchingin case the LLM does\n",
    "            not output parseable JSON. Useful for evaluating the model before finetuning.\n",
    "            Defaults to \"\" (no passage); in this case the backup prediction will be \"\".\n",
    "\n",
    "    Returns:\n",
    "        str: option (a letter or \"\") predicted by the model.\n",
    "    \"\"\"\n",
    "    json_text = pred_text\n",
    "    if match := re.search(r\"\\{\", json_text):  # remove anything before first {\n",
    "        json_text = json_text[match.start() :]\n",
    "    if match := re.search(r\"\\}\", json_text[::-1]):  # remove anything after last }\n",
    "        json_text = json_text[: len(json_text) - match.start()]\n",
    "\n",
    "    try:\n",
    "        if match := re.match(r\"^[a-eA-E]$\", json.loads(json_text)[\"option\"].strip()):\n",
    "            return match.group(0)\n",
    "    except (json.JSONDecodeError, KeyError):\n",
    "        pass\n",
    "\n",
    "    # backup: if a passage is supplied, get the best match\n",
    "    if passage:\n",
    "        return get_best_match(pred_text, passage)\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "display(parse_prediction(example_pred_text))\n",
    "display(parse_prediction(example_substr, example_passage))\n",
    "display(parse_prediction('{\"option\": \"A - Early disseminated Lyme disease\"}', example_passage))\n",
    "display(parse_prediction(example_substr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33.3\n"
     ]
    }
   ],
   "source": [
    "from typing import Callable\n",
    "from transformers.modeling_utils import PreTrainedModel\n",
    "\n",
    "def batch_get_preds(model: PreTrainedModel, model_name: str) -> Callable[[dict], dict]:\n",
    "    def _get_preds(samples: dict) -> dict:\n",
    "        input_ids = tokenizer.apply_chat_template(\n",
    "            [[{\"role\": \"user\", \"content\": text}] for text in samples[\"input\"]],\n",
    "            add_generation_prompt=True,\n",
    "            padding=True,\n",
    "            return_tensors=\"pt\",\n",
    "        ).to(model.device)\n",
    "\n",
    "        output_ids = model.generate(\n",
    "            input_ids,\n",
    "            do_sample=True,\n",
    "            max_new_tokens=512,\n",
    "            temperature=1e-3,\n",
    "        )\n",
    "\n",
    "        pred_texts = get_generated_texts(input_ids, output_ids)\n",
    "        samples[f\"{model_name}_pred\"] = pred_texts\n",
    "        samples[f\"{model_name}_label\"] = pred_labels = [\n",
    "            parse_prediction(pred, text)\n",
    "            for text, pred in zip(samples[\"input\"], pred_texts)\n",
    "        ]\n",
    "        samples[f\"{model_name}_correct\"] = [\n",
    "            pred_label == true_label\n",
    "            for true_label, pred_label in zip(samples[\"true_label\"], pred_labels)\n",
    "        ]\n",
    "\n",
    "        return samples\n",
    "    return _get_preds\n",
    "\n",
    "test_set = dataset[\"test\"]\n",
    "test_set = test_set.map(batch_get_preds(model, \"gemma\"), batched=True, batch_size=BATCH_SIZE)\n",
    "gemma_accuracy = sum(test_set[\"gemma_correct\"]) / len(test_set)\n",
    "print(round(gemma_accuracy * 100, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['D', 'D', 'A']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set[\"true_label\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cloudspace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
