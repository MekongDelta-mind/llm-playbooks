{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "from tokenizers import Tokenizer, models, trainers, pre_tokenizers\n",
    "from transformers import AutoTokenizer, GPT2Config, GPT2LMHeadModel, Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "import zstandard as zstd\n",
    "import chess.pgn\n",
    "import io\n",
    "import re\n",
    "from torch.utils.data import DataLoader\n",
    "import math\n",
    "from datasets import Dataset, DatasetDict\n",
    "from sklearn.model_selection import train_test_split\n",
    "import stockfish\n",
    "import chess\n",
    "import chess.engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# READ DATA\n",
    "\n",
    "note to reader: this is where you can determine data size by choosing N when running read_first_n_games_as_strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure special moves and pieces are followed by a space\n",
    "def format_pgn(text):\n",
    "\n",
    "    # Remove metadata lines (anything within square brackets)\n",
    "    text = re.sub(r'\\[.*?\\]', '', text)\n",
    "\n",
    "    # Remove excessive spaces and newlines\n",
    "    text = ' '.join(text.split())\n",
    "\n",
    "    text = re.sub(r'\\{.*?\\}', '', text)  # Remove `{ ... }`\n",
    "    text = ' '.join(text.split())  # Normalize spaces \n",
    "    # Define patterns for different components\n",
    "    move_number_pattern = re.compile(r'(\\d+\\.)')  # Move numbers (e.g., \"1.\")\n",
    "    piece_pattern = re.compile(r'([KQRBN])')  # Chess pieces (e.g., \"N\", \"K\")\n",
    "    square_pattern = re.compile(r'([a-h][1-8])')  # Board squares (e4, d5, etc.)\n",
    "    special_move_pattern = re.compile(r'(O-O|O-O-O|\\+|#|x|=Q|=R|=B|=N)')  # Castling, check, capture, promotions\n",
    "    \n",
    "    # Ensure move numbers, pieces, and special moves are space-separated\n",
    "    text = move_number_pattern.sub(r'\\1 ', text)  # Move number spacing\n",
    "    text = piece_pattern.sub(r'\\1 ', text)  # Piece spacing\n",
    "    text = special_move_pattern.sub(r' \\1 ', text)  # Special moves spacing\n",
    "    \n",
    "    return ' '.join(text.split()) \n",
    "\n",
    "\n",
    "def read_first_n_games_as_strings(file_path, n=100):\n",
    "    with open(file_path, 'rb') as f:\n",
    "        dctx = zstd.ZstdDecompressor()\n",
    "        decompressed = dctx.stream_reader(f)\n",
    "        pgn_text = io.TextIOWrapper(decompressed, encoding='utf-8')\n",
    "\n",
    "        games = []\n",
    "        for _ in range(n):\n",
    "            game = chess.pgn.read_game(pgn_text)\n",
    "            if game is None:\n",
    "                break  # Stop if no more games are available\n",
    "            \n",
    "            # Convert game to string\n",
    "            game_str = io.StringIO()\n",
    "            game.accept(chess.pgn.StringExporter())  # Corrected line\n",
    "            games.append(format_pgn(str(game)))\n",
    "\n",
    "    return games\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREP DATA FOR TOKENIZER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. d4 1. .. N f6 2. f3 2. .. d5 3. c4 3. .. c6 4. e3 4. .. g6 5. N c3 5. .. B g7 6. B d3 6. .. O-O 7. N ge2 7. .. N a6 8. a3 8. .. N c7 9. b4 9. .. R e8 10. O-O 10. .. B d7 11. c5 11. .. e5 12. h3 12. .. e x d4 13. N x d4 13. .. N e6 14. R b1 14. .. N x d4 15. e x d4 15. .. N h5 16. N e2 16. .. Q f6 17. B c2 17. .. B f5 18. B x f5 18. .. Q x f5 19. B b2 19. .. N f4 20. N x f4 20. .. Q x f4 21. Q c2 21. .. R e3 22. B c1 22. .. B x d4 23. B x e3 23. .. B x e3 + 24. K h1 24. .. h5 25. Q e2 25. .. d4 26. R be1 26. .. R e8 27. Q d3 27. .. R e6 28. R e2 28. .. Q g3 29. R x e3 29. .. d x e3 30. Q d8 + 30. .. K g7 31. Q d3 31. .. e2 32. R g1 32. .. e1 =Q 33. R x e1 33. .. R x e1 + 0-1\n"
     ]
    }
   ],
   "source": [
    "# Replace with your file path\n",
    "file_path = \"/Users/nourfahmy/Documents/GitHub/llm-playbooks/lichess_db_standard_rated_2025-02.pgn.zst\"\n",
    "# Replace with number of games\n",
    "number_of_games = 1000\n",
    "games_as_strings = read_first_n_games_as_strings(file_path, n=number_of_games)\n",
    "\n",
    "# Print the first game's PGN as a string\n",
    "print(games_as_strings[0])\n",
    "\n",
    "# Convert to Hugging Face Dataset format\n",
    "dataset = Dataset.from_dict({\"text\": games_as_strings})\n",
    "\n",
    "train_data, val_data = train_test_split(dataset[\"text\"], test_size=0.2, random_state=42)\n",
    "\n",
    "train_dataset = Dataset.from_dict({\"text\": train_data})\n",
    "val_dataset = Dataset.from_dict({\"text\": val_data})\n",
    "\n",
    "dataset = DatasetDict({\n",
    "    \"train\": train_dataset,\n",
    "    \"validation\": val_dataset\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAIN TOKENIZER\n",
    "\n",
    "note to reader: if experimenting with model size, adjust tokenizer name accordingly below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# note to reader: if experimenting with model size, adjust tokenizer name accordingly below\n",
    "tokenizer_name = \"gpt2\"\n",
    "old_tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "\n",
    "tokenizer = old_tokenizer.train_new_from_iterator(dataset['train'], 120)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TOKENIZE DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize Function\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples['text'],\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        max_length=128,\n",
    "        return_special_tokens_mask=True\n",
    "    )\n",
    "\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "\n",
    "# Tokenize Dataset\n",
    "tokenized_datasets = dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=[\"text\"]\n",
    ")\n",
    "\n",
    "train_subset = tokenized_datasets[\"train\"]\n",
    "eval_subset = tokenized_datasets[\"validation\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SET UP AND CONFIGURE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 800/800 [00:00<00:00, 5239.28 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [00:00<00:00, 5181.06 examples/s]\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/var/folders/vn/ldnd29fd4v1dzjdkm158113w0000gn/T/ipykernel_4608/3809665167.py:56: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "  3%|â–Ž         | 10/300 [00:08<01:51,  2.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 5.2487, 'grad_norm': 69.58563995361328, 'learning_rate': 1.0000000000000002e-06, 'epoch': 0.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|â–‹         | 20/300 [00:11<01:28,  3.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.5979, 'grad_norm': 18.92144203186035, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|â–ˆ         | 30/300 [00:14<01:25,  3.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.7202, 'grad_norm': 10.164074897766113, 'learning_rate': 3e-06, 'epoch': 0.3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|â–ˆâ–Ž        | 40/300 [00:17<01:21,  3.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.2046, 'grad_norm': 6.215158939361572, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|â–ˆâ–‹        | 50/300 [00:20<01:19,  3.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8468, 'grad_norm': 10.298335075378418, 'learning_rate': 5e-06, 'epoch': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|â–ˆâ–ˆ        | 60/300 [00:24<01:16,  3.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.582, 'grad_norm': 6.750159740447998, 'learning_rate': 6e-06, 'epoch': 0.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|â–ˆâ–ˆâ–Ž       | 70/300 [00:27<01:13,  3.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.48, 'grad_norm': 19.53133201599121, 'learning_rate': 7.000000000000001e-06, 'epoch': 0.7}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|â–ˆâ–ˆâ–‹       | 80/300 [00:30<01:12,  3.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3502, 'grad_norm': 8.607635498046875, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|â–ˆâ–ˆâ–ˆ       | 90/300 [00:33<01:06,  3.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2761, 'grad_norm': 9.880142211914062, 'learning_rate': 9e-06, 'epoch': 0.9}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 100/300 [00:36<01:03,  3.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2495, 'grad_norm': 14.392038345336914, 'learning_rate': 1e-05, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 100/300 [00:39<01:03,  3.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.1386611461639404, 'eval_runtime': 2.315, 'eval_samples_per_second': 86.394, 'eval_steps_per_second': 10.799, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|â–ˆâ–ˆâ–ˆâ–‹      | 110/300 [00:43<01:09,  2.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1964, 'grad_norm': 9.96692180633545, 'learning_rate': 1.1000000000000001e-05, 'epoch': 1.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 120/300 [00:46<00:57,  3.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0956, 'grad_norm': 7.692927837371826, 'learning_rate': 1.2e-05, 'epoch': 1.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 130/300 [00:50<00:53,  3.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0917, 'grad_norm': 13.373727798461914, 'learning_rate': 1.3000000000000001e-05, 'epoch': 1.3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 140/300 [00:53<00:50,  3.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0676, 'grad_norm': 6.789460182189941, 'learning_rate': 1.4000000000000001e-05, 'epoch': 1.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 150/300 [00:56<00:47,  3.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0619, 'grad_norm': 6.120540142059326, 'learning_rate': 1.5e-05, 'epoch': 1.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 160/300 [00:59<00:44,  3.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0418, 'grad_norm': 9.279339790344238, 'learning_rate': 1.6000000000000003e-05, 'epoch': 1.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 170/300 [01:02<00:41,  3.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.055, 'grad_norm': 8.034231185913086, 'learning_rate': 1.7000000000000003e-05, 'epoch': 1.7}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 180/300 [01:05<00:37,  3.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0599, 'grad_norm': 6.392711639404297, 'learning_rate': 1.8e-05, 'epoch': 1.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 190/300 [01:09<00:34,  3.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9955, 'grad_norm': 5.997188091278076, 'learning_rate': 1.9e-05, 'epoch': 1.9}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 200/300 [01:12<00:31,  3.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0119, 'grad_norm': 8.70809268951416, 'learning_rate': 2e-05, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 200/300 [01:14<00:31,  3.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9783404469490051, 'eval_runtime': 2.2752, 'eval_samples_per_second': 87.903, 'eval_steps_per_second': 10.988, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 210/300 [01:18<00:31,  2.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9443, 'grad_norm': 7.077550888061523, 'learning_rate': 2.1e-05, 'epoch': 2.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 220/300 [01:21<00:25,  3.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9727, 'grad_norm': 4.930843353271484, 'learning_rate': 2.2000000000000003e-05, 'epoch': 2.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 230/300 [01:24<00:22,  3.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9475, 'grad_norm': 4.891750812530518, 'learning_rate': 2.3000000000000003e-05, 'epoch': 2.3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 240/300 [01:27<00:19,  3.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9692, 'grad_norm': 5.357297897338867, 'learning_rate': 2.4e-05, 'epoch': 2.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 250/300 [01:31<00:15,  3.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9084, 'grad_norm': 7.190113544464111, 'learning_rate': 2.5e-05, 'epoch': 2.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 260/300 [01:34<00:12,  3.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9326, 'grad_norm': 8.515923500061035, 'learning_rate': 2.6000000000000002e-05, 'epoch': 2.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 270/300 [01:37<00:09,  3.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9426, 'grad_norm': 4.5157551765441895, 'learning_rate': 2.7000000000000002e-05, 'epoch': 2.7}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 280/300 [01:40<00:06,  3.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9187, 'grad_norm': 6.070256233215332, 'learning_rate': 2.8000000000000003e-05, 'epoch': 2.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 290/300 [01:44<00:03,  3.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8628, 'grad_norm': 5.270162105560303, 'learning_rate': 2.9e-05, 'epoch': 2.9}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 300/300 [01:47<00:00,  3.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8922, 'grad_norm': 3.7519924640655518, 'learning_rate': 3e-05, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 300/300 [01:50<00:00,  3.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8538914322853088, 'eval_runtime': 2.293, 'eval_samples_per_second': 87.223, 'eval_steps_per_second': 10.903, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 300/300 [01:51<00:00,  2.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 111.2398, 'train_samples_per_second': 21.575, 'train_steps_per_second': 2.697, 'train_loss': 1.4174757766723634, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=300, training_loss=1.4174757766723634, metrics={'train_runtime': 111.2398, 'train_samples_per_second': 21.575, 'train_steps_per_second': 2.697, 'total_flos': 156775219200000.0, 'train_loss': 1.4174757766723634, 'epoch': 3.0})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create GPT model configuration\n",
    "# note to reader: you can modify configuration to see how it affects chess performance\n",
    "config = GPT2Config(\n",
    "    vocab_size=tokenizer.vocab_size,  # Match tokenizer's vocab size\n",
    "    n_positions=128,\n",
    "    n_embd=768,\n",
    "    n_layer=12,\n",
    "    n_head=12\n",
    ")\n",
    "\n",
    "# Initialize model\n",
    "model = GPT2LMHeadModel(config)\n",
    "\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False \n",
    ")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./gpt-chess\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=2,\n",
    "    warmup_steps=500,\n",
    "    logging_steps=10\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_subset,\n",
    "    eval_dataset=eval_subset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EVALUATE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChessSimulator(model, tokenizer, stockfish_filepath):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.board = chess.Board()\n",
    "        self.engine = chess.engine.SimpleEngine.popen_uci(stockfish_filepath)\n",
    "        self.prompt = '1. '\n",
    "        self.move_number = 1\n",
    "        self.modelTurn = True\n",
    "        self.currentMove = None\n",
    "    \n",
    "    def simulateGame(self):\n",
    "        while not self.board.is_checkmate():\n",
    "            try:\n",
    "                if self.modelTurn:\n",
    "                    self.generateModelMove()\n",
    "                else:\n",
    "                    self.generateChessMove()\n",
    "                self.modelTurn = not self.modelTurn\n",
    "            except:\n",
    "                return 0\n",
    "        self.engine.quit()\n",
    "\n",
    "        if self.board.is_checkmate():\n",
    "            if self.board.turn == chess.WHITE:\n",
    "                return 0\n",
    "            else: # counts draws as a win\n",
    "                return 1\n",
    "\n",
    "    def parseModelMove(self, move):\n",
    "        # Extract move using regex\n",
    "        match = re.search(r\"\\d+\\.\\s*([\\w\\d=+#-]+)\", move)\n",
    "        if match:\n",
    "            move = match.group(1)  # Capture only the move\n",
    "            print(\"Extracted Move:\", move)\n",
    "        self.board.push_san(move)\n",
    "\n",
    "    \n",
    "    def parseChessMove(self, move):\n",
    "        # meant to modify prompt\n",
    "        self.prompt += f' {self.move_number}... '\n",
    "        piece_moved = self.board.piece_at(move.from_square)\n",
    "        piece_name = piece_moved.symbol() # K Q B R N\n",
    "        if piece_name in ['K','Q','B','R','N']:\n",
    "            self.prompt += piece_name + ' '\n",
    "        self.prompt += chess.square_name(move.to_square)\n",
    "\n",
    "\n",
    "    def generateModelMove(self):\n",
    "        inputs = tokenizer(self.prompt, return_tensors=\"pt\")\n",
    "        outputs = model.generate(**inputs, max_length=6, num_return_sequences=1)\n",
    "        move = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        self.prompt = move\n",
    "        self.currentMove = self.parseModelMove(move)\n",
    "\n",
    "\n",
    "    def generateChessMove(self):\n",
    "        result = self.engine.play(self.board, chess.engine.Limit(time=2.0))  # Time limit for the move\n",
    "        self.board.push(result.move)\n",
    "        self.parseChessMove(result.move)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
